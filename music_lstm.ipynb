{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /Users/khalilbenayed/Google Drive/Courses/4A-S20/.venv/lib/python3.7/site-packages (5.7.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = []\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.midi\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes.txt', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "     # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "#     network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "#     network_input = network_input / float(n_vocab)\n",
    "\n",
    "#     network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return np.array(network_input), np.array(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs/MIDI-Unprocessed_SMF_22_R1_2004_01-04_ORIG_MID-AUDIO_22_R1_2004_05_Track05_wav.midi\n"
     ]
    }
   ],
   "source": [
    "notes = get_notes()\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "n_vocab = len(set(notes))\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "batches = get_batches(network_input, 10, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[61 81 50 72 81 50 61 81 50 72]\n",
      " [19 61 80 49 33 45 55 60 67 60]\n",
      " [10 80 19 43 60 28 42 45 59 71]\n",
      " [48 61 79 19 42 28 42 61 44 70]\n",
      " [42 72 37 10 31 10 50 61 81 19]\n",
      " [44 68 17 30  5 17 30 39 10 17]\n",
      " [71 60 80 65 45 69 60 33 52 43]\n",
      " [72 68 34 61 45 44 68  6 43 58]\n",
      " [29 59 60 48 70 55 48 59 47 59]\n",
      " [55 46 67 45 56 45 71 26 49 62]]\n",
      "\n",
      "y\n",
      " [[81 50 72 81 50 61 81 50 72 81]\n",
      " [61 80 49 33 45 55 60 67 60 72]\n",
      " [80 19 43 60 28 42 45 59 71 17]\n",
      " [61 79 19 42 28 42 61 44 70 17]\n",
      " [72 37 10 31 10 50 61 81 19 42]\n",
      " [68 17 30  5 17 30 39 10 17 39]\n",
      " [60 80 65 45 69 60 33 52 43 44]\n",
      " [68 34 61 45 44 68  6 43 58 61]\n",
      " [59 60 48 70 55 48 59 47 59 44]\n",
      " [46 67 45 56 45 71 26 49 62 47]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.char2int = dict((note, number) for number, note in enumerate(tokens))\n",
    "        self.int2char = {i: note for note, i in self.char2int.items()}\n",
    "        \n",
    "        ## Define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## Define a dropout layer\n",
    "#         self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## Define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hc`. '''\n",
    "        \n",
    "        ## Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        ## Ppass x through the dropout layer\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## Put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "                        \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    \n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(82, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=82, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and print the network\n",
    "net = CharRNN(pitchnames, n_hidden=512, n_layers=2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seqs, n_steps = 10, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalilbenayed/Google Drive/Courses/4A-S20/.venv/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/khalilbenayed/Google Drive/Courses/4A-S20/.venv/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 1... Loss: 4.4070... Val Loss: nan\n",
      "Epoch: 1/1... Step: 2... Loss: 4.1409... Val Loss: nan\n",
      "Epoch: 1/1... Step: 3... Loss: 3.9063... Val Loss: nan\n",
      "Epoch: 1/1... Step: 4... Loss: 3.9040... Val Loss: nan\n",
      "Epoch: 1/1... Step: 5... Loss: 3.7754... Val Loss: nan\n",
      "Epoch: 1/1... Step: 6... Loss: 3.7645... Val Loss: nan\n",
      "Epoch: 1/1... Step: 7... Loss: 3.7172... Val Loss: nan\n",
      "Epoch: 1/1... Step: 8... Loss: 3.7063... Val Loss: nan\n",
      "Epoch: 1/1... Step: 9... Loss: 3.6362... Val Loss: nan\n",
      "Epoch: 1/1... Step: 10... Loss: 3.6450... Val Loss: nan\n",
      "Epoch: 1/1... Step: 11... Loss: 3.5645... Val Loss: nan\n",
      "Epoch: 1/1... Step: 12... Loss: 3.5538... Val Loss: nan\n",
      "Epoch: 1/1... Step: 13... Loss: 3.4712... Val Loss: nan\n",
      "Epoch: 1/1... Step: 14... Loss: 3.4376... Val Loss: nan\n",
      "Epoch: 1/1... Step: 15... Loss: 3.3507... Val Loss: nan\n",
      "Epoch: 1/1... Step: 16... Loss: 3.2881... Val Loss: nan\n",
      "Epoch: 1/1... Step: 17... Loss: 3.2139... Val Loss: nan\n",
      "Epoch: 1/1... Step: 18... Loss: 3.2327... Val Loss: nan\n",
      "Epoch: 1/1... Step: 19... Loss: 3.1452... Val Loss: nan\n",
      "Epoch: 1/1... Step: 20... Loss: 3.0810... Val Loss: nan\n",
      "Epoch: 1/1... Step: 21... Loss: 3.0687... Val Loss: nan\n",
      "Epoch: 1/1... Step: 22... Loss: 3.0015... Val Loss: nan\n",
      "Epoch: 1/1... Step: 23... Loss: 2.9518... Val Loss: nan\n",
      "Epoch: 1/1... Step: 24... Loss: 2.9279... Val Loss: nan\n",
      "Epoch: 1/1... Step: 25... Loss: 2.8350... Val Loss: nan\n",
      "Epoch: 1/1... Step: 26... Loss: 2.8236... Val Loss: nan\n",
      "Epoch: 1/1... Step: 27... Loss: 2.7718... Val Loss: nan\n",
      "Epoch: 1/1... Step: 28... Loss: 2.7409... Val Loss: nan\n",
      "Epoch: 1/1... Step: 29... Loss: 2.7288... Val Loss: nan\n",
      "Epoch: 1/1... Step: 30... Loss: 2.6377... Val Loss: nan\n",
      "Epoch: 1/1... Step: 31... Loss: 2.6207... Val Loss: nan\n",
      "Epoch: 1/1... Step: 32... Loss: 2.5747... Val Loss: nan\n",
      "Epoch: 1/1... Step: 33... Loss: 2.5237... Val Loss: nan\n",
      "Epoch: 1/1... Step: 34... Loss: 2.4521... Val Loss: nan\n",
      "Epoch: 1/1... Step: 35... Loss: 2.4406... Val Loss: nan\n",
      "Epoch: 1/1... Step: 36... Loss: 2.3928... Val Loss: nan\n",
      "Epoch: 1/1... Step: 37... Loss: 2.4068... Val Loss: nan\n",
      "Epoch: 1/1... Step: 38... Loss: 2.2987... Val Loss: nan\n",
      "Epoch: 1/1... Step: 39... Loss: 2.3045... Val Loss: nan\n",
      "Epoch: 1/1... Step: 40... Loss: 2.2407... Val Loss: nan\n",
      "Epoch: 1/1... Step: 41... Loss: 2.2622... Val Loss: nan\n",
      "Epoch: 1/1... Step: 42... Loss: 2.1516... Val Loss: nan\n",
      "Epoch: 1/1... Step: 43... Loss: 2.1671... Val Loss: nan\n",
      "Epoch: 1/1... Step: 44... Loss: 2.0723... Val Loss: nan\n",
      "Epoch: 1/1... Step: 45... Loss: 2.1287... Val Loss: nan\n",
      "Epoch: 1/1... Step: 46... Loss: 2.0016... Val Loss: nan\n",
      "Epoch: 1/1... Step: 47... Loss: 2.0540... Val Loss: nan\n",
      "Epoch: 1/1... Step: 48... Loss: 1.9614... Val Loss: nan\n",
      "Epoch: 1/1... Step: 49... Loss: 1.9670... Val Loss: nan\n",
      "Epoch: 1/1... Step: 50... Loss: 1.8444... Val Loss: nan\n",
      "Epoch: 1/1... Step: 51... Loss: 1.9295... Val Loss: nan\n",
      "Epoch: 1/1... Step: 52... Loss: 1.7850... Val Loss: nan\n",
      "Epoch: 1/1... Step: 53... Loss: 1.8789... Val Loss: nan\n",
      "Epoch: 1/1... Step: 54... Loss: 1.7417... Val Loss: nan\n",
      "Epoch: 1/1... Step: 55... Loss: 1.7837... Val Loss: nan\n",
      "Epoch: 1/1... Step: 56... Loss: 1.6820... Val Loss: nan\n",
      "Epoch: 1/1... Step: 57... Loss: 1.8067... Val Loss: nan\n",
      "Epoch: 1/1... Step: 58... Loss: 1.6278... Val Loss: nan\n",
      "Epoch: 1/1... Step: 59... Loss: 1.6969... Val Loss: nan\n",
      "Epoch: 1/1... Step: 60... Loss: 1.5972... Val Loss: nan\n",
      "Epoch: 1/1... Step: 61... Loss: 1.6496... Val Loss: nan\n",
      "Epoch: 1/1... Step: 62... Loss: 1.4926... Val Loss: nan\n",
      "Epoch: 1/1... Step: 63... Loss: 1.5739... Val Loss: nan\n",
      "Epoch: 1/1... Step: 64... Loss: 1.4250... Val Loss: nan\n",
      "Epoch: 1/1... Step: 65... Loss: 1.5576... Val Loss: nan\n",
      "Epoch: 1/1... Step: 66... Loss: 1.3295... Val Loss: nan\n",
      "Epoch: 1/1... Step: 67... Loss: 1.5250... Val Loss: nan\n",
      "Epoch: 1/1... Step: 68... Loss: 1.2195... Val Loss: nan\n",
      "Epoch: 1/1... Step: 69... Loss: 1.5148... Val Loss: nan\n",
      "Epoch: 1/1... Step: 70... Loss: 1.3529... Val Loss: nan\n",
      "Epoch: 1/1... Step: 71... Loss: 1.4774... Val Loss: nan\n",
      "Epoch: 1/1... Step: 72... Loss: 1.2621... Val Loss: nan\n",
      "Epoch: 1/1... Step: 73... Loss: 1.4518... Val Loss: nan\n",
      "Epoch: 1/1... Step: 74... Loss: 1.2472... Val Loss: nan\n",
      "Epoch: 1/1... Step: 75... Loss: 1.3547... Val Loss: nan\n",
      "Epoch: 1/1... Step: 76... Loss: 1.1482... Val Loss: nan\n",
      "Epoch: 1/1... Step: 77... Loss: 1.2391... Val Loss: nan\n",
      "Epoch: 1/1... Step: 78... Loss: 1.1170... Val Loss: nan\n",
      "Epoch: 1/1... Step: 79... Loss: 1.2222... Val Loss: nan\n",
      "Epoch: 1/1... Step: 80... Loss: 1.0431... Val Loss: nan\n",
      "Epoch: 1/1... Step: 81... Loss: 1.1832... Val Loss: nan\n",
      "Epoch: 1/1... Step: 82... Loss: 0.9949... Val Loss: nan\n",
      "Epoch: 1/1... Step: 83... Loss: 1.1088... Val Loss: nan\n",
      "Epoch: 1/1... Step: 84... Loss: 0.8932... Val Loss: nan\n",
      "Epoch: 1/1... Step: 85... Loss: 1.0840... Val Loss: nan\n",
      "Epoch: 1/1... Step: 86... Loss: 0.9259... Val Loss: nan\n",
      "Epoch: 1/1... Step: 87... Loss: 1.1011... Val Loss: nan\n",
      "Epoch: 1/1... Step: 88... Loss: 0.8602... Val Loss: nan\n",
      "Epoch: 1/1... Step: 89... Loss: 1.0948... Val Loss: nan\n",
      "Epoch: 1/1... Step: 90... Loss: 0.8279... Val Loss: nan\n",
      "Epoch: 1/1... Step: 91... Loss: 1.0642... Val Loss: nan\n",
      "Epoch: 1/1... Step: 92... Loss: 0.8124... Val Loss: nan\n",
      "Epoch: 1/1... Step: 93... Loss: 0.9190... Val Loss: nan\n",
      "Epoch: 1/1... Step: 94... Loss: 0.7197... Val Loss: nan\n",
      "Epoch: 1/1... Step: 95... Loss: 0.8789... Val Loss: nan\n",
      "Epoch: 1/1... Step: 96... Loss: 0.7105... Val Loss: nan\n",
      "Epoch: 1/1... Step: 97... Loss: 1.0076... Val Loss: nan\n",
      "Epoch: 1/1... Step: 98... Loss: 0.7097... Val Loss: nan\n",
      "Epoch: 1/1... Step: 99... Loss: 0.8351... Val Loss: nan\n",
      "Epoch: 1/1... Step: 100... Loss: 0.7534... Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "train(net, network_input, epochs=1, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime=notes[:2], top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        print(ch)\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        \n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5\n",
      "G5\n"
     ]
    }
   ],
   "source": [
    "generated_notes = sample(net, 2000, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'A4',\n",
       " '2.7',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'A4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " '7.11',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'D4',\n",
       " 'B4',\n",
       " 'F#4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B5',\n",
       " '7.11',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " '2.7',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " '9.1',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " '9',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'B3',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " '9.1',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " '9',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'B3',\n",
       " 'B3',\n",
       " 'G4',\n",
       " '2.7',\n",
       " 'D5',\n",
       " '9.2',\n",
       " '6',\n",
       " '9.2',\n",
       " 'D5',\n",
       " 'D5',\n",
       " '2.6',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " '7.11',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'A4',\n",
       " '7.11.2',\n",
       " 'B4',\n",
       " '0.2',\n",
       " 'D5',\n",
       " '2.7',\n",
       " 'B4',\n",
       " '2.6.9',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " '0',\n",
       " '4.7',\n",
       " '4.7',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " '2.7',\n",
       " '2.7',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " '7.11',\n",
       " '7.11',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'E4',\n",
       " 'E5',\n",
       " '4.7',\n",
       " 'E5',\n",
       " 'F#5',\n",
       " '0.2',\n",
       " '1.4',\n",
       " '2.6.9',\n",
       " '2.7',\n",
       " 'G5',\n",
       " '11.2',\n",
       " 'B4',\n",
       " '11.2',\n",
       " 'D5',\n",
       " 'G5',\n",
       " '2.7',\n",
       " 'C5',\n",
       " '6',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " '2.6',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'D5',\n",
       " '0.2',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'F#5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " '7.11',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'F#4',\n",
       " 'D4',\n",
       " 'G4',\n",
       " 'F#4',\n",
       " 'B3',\n",
       " 'G4',\n",
       " 'D4',\n",
       " '2.7',\n",
       " '2.7',\n",
       " '11.2',\n",
       " '2.6.9',\n",
       " '11.2',\n",
       " '2.7',\n",
       " 'B4',\n",
       " '11.2',\n",
       " '2.6.9',\n",
       " 'A2',\n",
       " 'G5',\n",
       " '9.2',\n",
       " 'A3',\n",
       " 'C4',\n",
       " '4.7',\n",
       " 'F#5',\n",
       " 'C6',\n",
       " '9.2',\n",
       " '6.9',\n",
       " 'E5',\n",
       " 'D5',\n",
       " '2.6',\n",
       " 'A5',\n",
       " '9.1',\n",
       " '11.2',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " '9.2',\n",
       " 'D5',\n",
       " 'D3',\n",
       " 'G4',\n",
       " '9.2',\n",
       " 'D5',\n",
       " 'F#3',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " '0.2',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " '7.11',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " '2.7',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'A5',\n",
       " 'B5',\n",
       " 'A5',\n",
       " 'G4',\n",
       " 'G4',\n",
       " 'A4',\n",
       " 'D3',\n",
       " 'B3',\n",
       " 'F#4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G3',\n",
       " 'C5',\n",
       " 'A3',\n",
       " 'G3',\n",
       " 'G3',\n",
       " 'A3',\n",
       " 'D4',\n",
       " 'F#4',\n",
       " 'F#4',\n",
       " 'E3',\n",
       " 'F#3',\n",
       " '6.9',\n",
       " 'F#3',\n",
       " 'F#3',\n",
       " '9.1.4',\n",
       " '11.2',\n",
       " '11.2',\n",
       " 'D3',\n",
       " '2.7',\n",
       " '11.2',\n",
       " '2.7',\n",
       " 'A2',\n",
       " '9.2',\n",
       " '6',\n",
       " 'A4',\n",
       " 'A3',\n",
       " 'F#4',\n",
       " '2.6',\n",
       " 'E4',\n",
       " 'G2',\n",
       " 'A2',\n",
       " '9.1',\n",
       " '11.2',\n",
       " 'B2',\n",
       " 'G2',\n",
       " 'B4',\n",
       " 'F#2',\n",
       " 'A4',\n",
       " 'G2',\n",
       " 'A2',\n",
       " 'D4',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'F#2',\n",
       " 'A4',\n",
       " 'G2',\n",
       " 'A2',\n",
       " 'D4',\n",
       " 'E4',\n",
       " 'G2',\n",
       " 'E2',\n",
       " 'B2',\n",
       " 'F#2',\n",
       " 'A4',\n",
       " 'G2',\n",
       " 'A2',\n",
       " 'E4',\n",
       " 'F#2',\n",
       " 'E2',\n",
       " 'G4',\n",
       " 'E2',\n",
       " 'F#2',\n",
       " 'A4',\n",
       " 'G2',\n",
       " 'C#4',\n",
       " 'D4',\n",
       " 'E4',\n",
       " 'E4',\n",
       " 'C#3',\n",
       " 'B2',\n",
       " 'A3',\n",
       " 'E4',\n",
       " '2.6',\n",
       " 'C#3',\n",
       " 'E4',\n",
       " 'A4',\n",
       " 'A4',\n",
       " 'E4',\n",
       " 'C#5',\n",
       " 'E4',\n",
       " 'G5',\n",
       " 'B3',\n",
       " 'E4',\n",
       " 'G5',\n",
       " 'B3',\n",
       " 'F#3',\n",
       " 'D3',\n",
       " 'F#3',\n",
       " 'F#3',\n",
       " '9.1.4',\n",
       " '2.6.9',\n",
       " 'F#3',\n",
       " '9.2',\n",
       " 'F#3',\n",
       " '9.2',\n",
       " 'F#3',\n",
       " '9.1.4',\n",
       " '9.2',\n",
       " 'F#3',\n",
       " 'F#3',\n",
       " '9.1.4',\n",
       " 'F#3',\n",
       " 'D3',\n",
       " 'D3',\n",
       " 'E3',\n",
       " 'D3',\n",
       " 'E3',\n",
       " '6.9',\n",
       " '7.11',\n",
       " '9.1',\n",
       " '11.2',\n",
       " '2.6',\n",
       " '6.9',\n",
       " '7.11',\n",
       " '7.11',\n",
       " '9.1',\n",
       " '1.4',\n",
       " '2.6',\n",
       " 'E5',\n",
       " '4.7',\n",
       " '9.1',\n",
       " '11.2',\n",
       " '2.6',\n",
       " 'F#5',\n",
       " '9.1',\n",
       " '1.4',\n",
       " '2.6',\n",
       " '1.4',\n",
       " 'A5',\n",
       " 'A5',\n",
       " 'C#4',\n",
       " 'A4',\n",
       " 'G4',\n",
       " 'C#5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'A2',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'C#4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " '7.11',\n",
       " 'B4',\n",
       " '7.11',\n",
       " '2.6.9',\n",
       " 'B4',\n",
       " '7.11.2',\n",
       " 'B4',\n",
       " '0.2',\n",
       " '11.2',\n",
       " '2.7',\n",
       " 'B4',\n",
       " '2.6.9',\n",
       " '2.7',\n",
       " '11.2',\n",
       " 'B4',\n",
       " '2.6.9',\n",
       " 'G5',\n",
       " 'C4',\n",
       " '4.7',\n",
       " 'C4',\n",
       " '4.7',\n",
       " 'G4',\n",
       " 'F#5',\n",
       " 'C5',\n",
       " 'C6',\n",
       " '11',\n",
       " '2.7',\n",
       " '11',\n",
       " 'D5',\n",
       " 'G4',\n",
       " '7.11',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'A4',\n",
       " 'E4',\n",
       " 'D4',\n",
       " 'F#5',\n",
       " 'D4',\n",
       " 'E4',\n",
       " 'B3',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'A4',\n",
       " 'C5',\n",
       " 'D4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " '7.11',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'A5',\n",
       " 'G4',\n",
       " '9.1',\n",
       " 'B4',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " '2.7',\n",
       " 'D5',\n",
       " 'B4',\n",
       " '2.6.9',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " '9.2',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " '2.7',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'A4',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'D5',\n",
       " 'B5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'C5',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'A5',\n",
       " 'G5',\n",
       " 'G4',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " '9.1',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " '9',\n",
       " '9.2',\n",
       " '9',\n",
       " 'A5',\n",
       " '9.2',\n",
       " 'F#5',\n",
       " 'D5',\n",
       " 'B3',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'G4',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'A4',\n",
       " 'B4',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G5',\n",
       " 'B5',\n",
       " 'D5',\n",
       " 'G5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'E5',\n",
       " 'C5',\n",
       " 'D5',\n",
       " 'B4',\n",
       " 'G4',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in generated_notes:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_output.midi', 'w') as f:\n",
    "    f.write(generated_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
